# Machine learning in production

The first few steps are the same as in the basic workflow described in section
[101.4](./101/4.md), but if you want to use machine learning in production there
are some extra things that you need to think about.

As mentioned in [101.4](./101/4.md) some of these workflow steps are cyclical,
so this will be discussed here too.

(This is also an opportunity to mention that Infinity Works can help clients 
to do all of this stuff!) 

1. Define the business problem

2. Collect relevant data

3. Data exploration and cleaning

4. Feature engineering

5. Training

6. Model evaluation

7. Tuning
    - Explain what hyperparameters are, and why and how we tune them.
    - Discuss search strategies, e.g. grid search.

8. Deployment
    - Try to deploy the model that was trained, don't chuck it over the wall to another team to put it in production! Don't re-write models to make them more performant - this approach used to be used a lot but involves a lot of duplicated work and can introduce subtle bugs, and reduces the team's velocity.
    - How do I know how my model performs relative to previous versions? A/B testing is important.
    - Is the data that is being fed to the model for inference similar to the training data? Data can drift, so ideally you would do some statistical analysis of the incoming data and check its similarity to the training set.
    - Version tracking - some regulatory environments require this - if we want to replay a request to investigate why the model made the inference it did, we need:
        - Request payload
        - Model deployment timestamp
        - A record of which model responded to each inference request; multiple versions may have been running, e.g. in an A/B test
        - Code version that was used to build the model
        - Model version and deployment artifacts
        - Training data version - this may be hard if TB of data was used to train the model.

9. Inference
    - How will you provide enough compute to run inference in production?

10. Performance monitoring & alerting
    - Log all requests and inference results. This allows you to detect drift in data distributions.
    - Predictive performance can decay because the world changes, so models will need to be re-trained periodically. How will you decide when, and what does this process look like?
    - How will you scale your monitoring thousands of models?

## Model lifecycle management tooling
- [Datatron](https://www.datatron.com/)
- [Pachyderm](https://www.pachyderm.io/)
- [Kubeflow](https://www.kubeflow.org/)
- [Apache Airflow](https://airflow.apache.org/)
- [Algorithmia](https://algorithmia.com)
- [Numericcal](https://www.numericcal.com)
- [Seldon](https://www.seldon.io)
- [Datmo](https://www.datmo.com/)
- [DVC](https://dvc.org/)
- [MLFlow](https://mlflow.org)
- [MLPerf](https://mlperf.org)
- [Neptune](https://neptune.ml)
- [DataRobot](https://www.datarobot.com/)
- [Xyonix](https://www.xyonix.com/)
- [Paperspace Gradient](https://gradient.paperspace.com/)
- [Polyaxon](https://polyaxon.com/)
